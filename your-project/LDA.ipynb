{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "    \n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import corpus\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "from nltk.stem import PorterStemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += ['from', 'subject', 're', 'edu', 'use','user', 'com', 'co', 'con', 'be', 'else', 'http', 'would','send', \n",
    "                   'do', 'try', 'tell', 'go', 'get', 'can', 'think', 'know', 'give', 'ask', \n",
    "               'next', 'find', 're']\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word.strip() not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def stem_and_lemmatize(tweet):\n",
    "    tweet = ' '.join(tweet)\n",
    "    stem = PorterStemmer().stem(tweet)\n",
    "    return WordNetLemmatizer().lemmatize(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_csv('cdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none             0.901269\n",
       "Boris Johnson    0.053743\n",
       "Theresa May      0.044988\n",
       "Name: PM, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf['PM'].value_counts()/len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305304, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing dataset from 300k to 30k (removing tweets that do not refer to either Boris or Theresa May)\n",
    "df = cdf.drop(cdf[(cdf.PM == 'none')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30143, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping nan values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boris Johnson    16407\n",
       "Theresa May      13735\n",
       "Name: PM, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "df.drop_duplicates(subset='TEXT', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH_INT</th>\n",
       "      <th>MONTH_STR</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TWEET_PROCESSED</th>\n",
       "      <th>TWEET_CLEANED</th>\n",
       "      <th>PM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Macetrain</td>\n",
       "      <td>Nicola Sturgeon tells Boris Johnson: Brexit of...</td>\n",
       "      <td>['nicola', 'sturgeon', 'tells', 'boris', 'john...</td>\n",
       "      <td>nicola sturgeon tells boris johnson brexit off...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>DerekJa09788684</td>\n",
       "      <td>If we ever get brexit.and Boris is looking mor...</td>\n",
       "      <td>['ever', 'brexit', 'boris', 'looking', 'dodgy'...</td>\n",
       "      <td>ever brexit boris looking dodgy continental qu...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>johnleremainer</td>\n",
       "      <td>Media have finally realised that Brexit will b...</td>\n",
       "      <td>['media', 'finally', 'realised', 'brexit', 'di...</td>\n",
       "      <td>media finally realised brexit disaster johnson...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Endoxa66</td>\n",
       "      <td>@BorisJohnson this better be a joke or else yo...</td>\n",
       "      <td>['borisjohnson', 'better', 'joke', 'else', 'to...</td>\n",
       "      <td>borisjohnson better joke else tories toast bre...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>KamalJoshi108</td>\n",
       "      <td>\"Turbo-charge\" Brexit plans, with \"all necessa...</td>\n",
       "      <td>['turbo', 'charge', 'brexit', 'plans', 'necess...</td>\n",
       "      <td>turbo charge brexit plans necessary funding pr...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305173</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>guardian</td>\n",
       "      <td>The Guardian view on Boris Johnson in court: B...</td>\n",
       "      <td>['guardian', 'view', 'boris', 'johnson', 'cour...</td>\n",
       "      <td>guardian view boris johnson court brexit editori</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305174</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Col_Bogey</td>\n",
       "      <td>Johnson is Trump's poodle -- a weak, unpatriot...</td>\n",
       "      <td>['johnson', 'trump', 'poodle', 'weak', 'unpatr...</td>\n",
       "      <td>johnson trump poodle weak unpatriotic irritati...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305179</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>PoetintheWoods</td>\n",
       "      <td>Poet in the Woods: The Voice of Many? https://...</td>\n",
       "      <td>['poet', 'woods', 'voice', 'many', 'brexit', '...</td>\n",
       "      <td>poet woods voice many brexit boris muddling co...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305213</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>cpseed</td>\n",
       "      <td>I don't need to learn anything about Brexit, I...</td>\n",
       "      <td>['need', 'learn', 'anything', 'brexit', 'fully...</td>\n",
       "      <td>need learn anything brexit fully understand tr...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305222</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>TheTrut27638359</td>\n",
       "      <td>BBC News - Brexit: EU Parliament makes tough d...</td>\n",
       "      <td>['news', 'brexit', 'parliament', 'makes', 'tou...</td>\n",
       "      <td>news brexit parliament makes tough demands tal...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28166 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MONTH_INT MONTH_STR         USERNAME  \\\n",
       "7               7       Jul        Macetrain   \n",
       "9               7       Jul  DerekJa09788684   \n",
       "12              7       Jul   johnleremainer   \n",
       "22              7       Jul         Endoxa66   \n",
       "24              7       Jul    KamalJoshi108   \n",
       "...           ...       ...              ...   \n",
       "305173          2       Feb         guardian   \n",
       "305174          2       Feb        Col_Bogey   \n",
       "305179          2       Feb   PoetintheWoods   \n",
       "305213          2       Feb           cpseed   \n",
       "305222          2       Feb  TheTrut27638359   \n",
       "\n",
       "                                                     TEXT  \\\n",
       "7       Nicola Sturgeon tells Boris Johnson: Brexit of...   \n",
       "9       If we ever get brexit.and Boris is looking mor...   \n",
       "12      Media have finally realised that Brexit will b...   \n",
       "22      @BorisJohnson this better be a joke or else yo...   \n",
       "24      \"Turbo-charge\" Brexit plans, with \"all necessa...   \n",
       "...                                                   ...   \n",
       "305173  The Guardian view on Boris Johnson in court: B...   \n",
       "305174  Johnson is Trump's poodle -- a weak, unpatriot...   \n",
       "305179  Poet in the Woods: The Voice of Many? https://...   \n",
       "305213  I don't need to learn anything about Brexit, I...   \n",
       "305222  BBC News - Brexit: EU Parliament makes tough d...   \n",
       "\n",
       "                                          TWEET_PROCESSED  \\\n",
       "7       ['nicola', 'sturgeon', 'tells', 'boris', 'john...   \n",
       "9       ['ever', 'brexit', 'boris', 'looking', 'dodgy'...   \n",
       "12      ['media', 'finally', 'realised', 'brexit', 'di...   \n",
       "22      ['borisjohnson', 'better', 'joke', 'else', 'to...   \n",
       "24      ['turbo', 'charge', 'brexit', 'plans', 'necess...   \n",
       "...                                                   ...   \n",
       "305173  ['guardian', 'view', 'boris', 'johnson', 'cour...   \n",
       "305174  ['johnson', 'trump', 'poodle', 'weak', 'unpatr...   \n",
       "305179  ['poet', 'woods', 'voice', 'many', 'brexit', '...   \n",
       "305213  ['need', 'learn', 'anything', 'brexit', 'fully...   \n",
       "305222  ['news', 'brexit', 'parliament', 'makes', 'tou...   \n",
       "\n",
       "                                            TWEET_CLEANED             PM  \n",
       "7       nicola sturgeon tells boris johnson brexit off...  Boris Johnson  \n",
       "9       ever brexit boris looking dodgy continental qu...  Boris Johnson  \n",
       "12      media finally realised brexit disaster johnson...  Boris Johnson  \n",
       "22      borisjohnson better joke else tories toast bre...  Boris Johnson  \n",
       "24      turbo charge brexit plans necessary funding pr...  Boris Johnson  \n",
       "...                                                   ...            ...  \n",
       "305173   guardian view boris johnson court brexit editori  Boris Johnson  \n",
       "305174  johnson trump poodle weak unpatriotic irritati...  Boris Johnson  \n",
       "305179  poet woods voice many brexit boris muddling co...  Boris Johnson  \n",
       "305213  need learn anything brexit fully understand tr...  Boris Johnson  \n",
       "305222  news brexit parliament makes tough demands tal...  Boris Johnson  \n",
       "\n",
       "[28166 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby your key and freq\n",
    "g = df.groupby(pd.Grouper(key='MONTH_INT'))\n",
    "# groups to a list of dataframes with list comprehension\n",
    "dfss = [group for _,group in g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = df.MONTH_INT.unique()\n",
    "monthsdict = {elem : pd.DataFrame() for elem in months}\n",
    "for key in monthsdict.keys():\n",
    "    monthsdict[key] = df[:][df.MONTH_INT == key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = ['df_jan', 'df_feb', 'df_march', 'df_april', 'df_may', 'df_june', 'df_july', 'df_aug', 'df_sep',\n",
    "           'df_oct', 'df_nov', 'df_dec']        #a list of all the dataframes you want to create\n",
    "for name in df:\n",
    "    monthsdict[name] = df(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan = df[(df.MONTH_STR == 'Jan')]\n",
    "df_fev = df[(df.MONTH_STR == 'Fev')]\n",
    "df_march = df[(df.MONTH_STR == 'Mar')]\n",
    "df_april = df[(df.MONTH_STR == 'Apr')]\n",
    "df_may = df[(df.MONTH_STR == 'May')]\n",
    "df_june = df[(df.MONTH_STR == 'Jun')] \n",
    "df_july = df[(df.MONTH_STR == 'Jul')]\n",
    "df_aug = df[(df.MONTH_STR == 'Aug')] \n",
    "df_sep = df[(df.MONTH_STR == 'Sep')] \n",
    "df_oct = df[(df.MONTH_STR == 'Oct')] \n",
    "df_nov = df[(df.MONTH_STR == 'Nov')] \n",
    "df_dec = df[(df.MONTH_STR == 'Dec')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_march = pd.concat([df_jan,df_fev,df_march,df_april])\n",
    "df_june = pd.concat([df_may,df_june,df_july, df_aug])\n",
    "df_sep = pd.concat([df_sep,df_oct,df_nov, df_dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apr    3604\n",
       "Dec    3149\n",
       "Oct    3052\n",
       "Mar    2964\n",
       "Jan    2741\n",
       "Nov    2415\n",
       "Sep    2115\n",
       "Jul    2031\n",
       "Feb    1879\n",
       "Jun    1555\n",
       "Aug    1479\n",
       "May    1182\n",
       "Name: MONTH_STR, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.MONTH_STR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the series into lists for text processing\n",
    "df_march = df_march['TWEET_CLEANED'].tolist()\n",
    "df_july = df_july['TWEET_CLEANED'].tolist()\n",
    "df_nov = df_nov['TWEET_CLEANED'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### January - April Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams and trigrams\n",
    "bigram = gensim.models.Phrases(df_march, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.Phrases(bigram[df_march], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_march)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_m = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['upholding', 'rule', 'crime', 'laughable', 'brexitshamble']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_m[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_m = corpora.Dictionary(data_lemmatized_m)\n",
    "id2word_m.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_m = data_lemmatized_m\n",
    "# Term Document Frequency\n",
    "corpus_m = [id2word_m.doc2bow(text) for text in texts_m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May - August Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams and trigrams\n",
    "bigram = gensim.models.Phrases(df_july, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.Phrases(bigram[df_july], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_july)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_j = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['offer']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_j[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_j = corpora.Dictionary(data_lemmatized_j)\n",
    "id2word_j.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_j = data_lemmatized_j\n",
    "# Term Document Frequency\n",
    "corpus_j = [id2word_j.doc2bow(text) for text in texts_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September - December Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams and trigrams\n",
    "bigram = gensim.models.Phrases(df_nov, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.Phrases(bigram[df_nov], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_nov)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_n = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB', 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['check', 'remainer', 'nee', 'hold', 'nose', 'good', 'judgement', 'stop', 'get', 'majority', 'ensure', 'deal', 'brexit', 'good', 'site', 'compare', 'tactical', 'voting', 'sit']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_n[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_n = corpora.Dictionary(data_lemmatized_n)\n",
    "id2word_n.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_n = data_lemmatized_n\n",
    "# Term Document Frequency\n",
    "corpus_n = [id2word_n.doc2bow(text) for text in texts_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF - not using\n",
    "\n",
    "A problem with this approach is that highly frequent words start to dominate in the document, but may not be representative to the model as less-frequent words. One way to fix this is to measure how unique (or how infrequent) a word is across all documents (or tweets), which is called the “inverse document frequency” or IDF. By introducing IDF, frequent words that are also frequent across all documents get penalized with less weight.\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-of-2019-hr-tech-conference-twitter-d16cf75895b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = models.TfidfModel(corpus_m)\n",
    "#tfidf_corpus_m = tfidf[corpus_m]\n",
    "\n",
    "#tfidf = models.TfidfModel(corpus_j)\n",
    "#tfidf_corpus_j = tfidf[corpus_j]\n",
    "\n",
    "#tfidf = models.TfidfModel(corpus_n)\n",
    "#tfidf_corpus_n = tfidf[corpus_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Via LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jan - April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_m = gensim.models.ldamodel.LdaModel(corpus=corpus_m,\n",
    "                                           id2word=id2word_m,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: talk|good|british|agree|look|right|last|work|sign|open|keep|word|promise|destroy|agreement|control|well|feel|hold|part\n",
      "Topic: 1 \n",
      "Words: vote|conservative|tory|party|remain|could|election|polling|government|face|claim|come|today|delay|call|thank|voter|parliament|fail|break\n",
      "Topic: 2 \n",
      "Words: brexit|say|leave|labour|deliver|country|stop|trade|change|thing|force|stay|bad|show|month|political|letter|news|poor|become\n",
      "Topic: 3 \n",
      "Words: deal|people|make|time|support|leader|need|must|lie|happen|lose|referendum|plan|turn|democracy|resign|jeremycorbyn|public|new|woman\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_m.show_topics(formatted=False, num_words= 20):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May - August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_j = gensim.models.ldamodel.LdaModel(corpus=corpus_j,\n",
    "                                           id2word=id2word_j,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: stop|become|leader|political|much|today|point|warn|great|push\n",
      "Topic: 1 \n",
      "Words: leave|borisjohnson|remain|happen|theresa|well|plan|must|support|really\n",
      "Topic: 2 \n",
      "Words: deal|brexit|vote|tory|say|party|make|people|even|country\n",
      "Topic: 3 \n",
      "Words: deliver|election|conservative|call|good|fail|right|promise|hard|thing\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_j.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September - December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_n = gensim.models.ldamodel.LdaModel(corpus=corpus_n,\n",
    "                                           id2word=id2word_n,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: agree|plan|policy|option|split|become|risk|seem|great|face|back|today|enough|turn|table|fall|clear|block|live|feel|accept|part|idea|ditch|fool|different|truth|big|side|explain\n",
      "Topic: 1 \n",
      "Words: need|seat|majority|lose|work|government|candidate|win|real|debate|pact|choose|power|let|move|problem|fight|form|remember|leaver|blame|break|clean|generalelection|company|serious|beat|fake|hand|possible\n",
      "Topic: 2 \n",
      "Words: remain|keep|forget|chance|country|remainer|look|must|full|brexiteer|well|position|force|public|fail|thank|wrong|report|parti|increase|dem|bori|surrender|bring|politic|russian|put|corrupt|police|one\n",
      "Topic: 3 \n",
      "Words: deal|vote|tory|brexit|party|leave|people|election|stop|say|farage|labour|conservative|make|deliver|happen|good|could|lie|time|campaign|come|borisjohnson|stand|voter|trade|support|parliament|thing|read\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_n.show_topics(formatted=False, num_words= 20):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4009366399074937\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_n, texts=data_lemmatized_n, dictionary=id2word_n, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el978817771997930887659777091\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el978817771997930887659777091_data = {\"mdsDat\": {\"x\": [-0.37359854968650946, 0.11496715738691823, 0.36039256875343306, -0.10176117645384194], \"y\": [0.10570776164905575, 0.35497837128594134, -0.11878771365109934, -0.34189841928389775], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [62.71646499633789, 13.516608238220215, 13.024070739746094, 10.742850303649902]}, \"tinfo\": {\"Term\": [\"need\", \"deal\", \"remain\", \"vote\", \"seat\", \"majority\", \"lose\", \"tory\", \"brexit\", \"agree\", \"keep\", \"forget\", \"work\", \"plan\", \"chance\", \"country\", \"remainer\", \"party\", \"leave\", \"policy\", \"look\", \"government\", \"must\", \"candidate\", \"win\", \"option\", \"split\", \"real\", \"full\", \"brexiteer\", \"deal\", \"vote\", \"tory\", \"brexit\", \"party\", \"leave\", \"people\", \"election\", \"say\", \"stop\", \"labour\", \"farage\", \"conservative\", \"make\", \"deliver\", \"good\", \"happen\", \"could\", \"lie\", \"time\", \"campaign\", \"come\", \"borisjohnson\", \"stand\", \"voter\", \"trade\", \"support\", \"parliament\", \"thing\", \"read\", \"remain\", \"keep\", \"forget\", \"chance\", \"country\", \"remainer\", \"look\", \"must\", \"full\", \"brexiteer\", \"well\", \"position\", \"public\", \"force\", \"fail\", \"thank\", \"wrong\", \"report\", \"dem\", \"increase\", \"parti\", \"bring\", \"surrender\", \"bori\", \"politic\", \"russian\", \"put\", \"police\", \"corrupt\", \"important\", \"one\", \"need\", \"seat\", \"majority\", \"lose\", \"work\", \"government\", \"candidate\", \"win\", \"real\", \"debate\", \"pact\", \"choose\", \"power\", \"move\", \"let\", \"fight\", \"problem\", \"form\", \"remember\", \"leaver\", \"break\", \"blame\", \"generalelection\", \"clean\", \"company\", \"hand\", \"beat\", \"place\", \"serious\", \"possible\", \"fake\", \"agree\", \"plan\", \"policy\", \"option\", \"split\", \"become\", \"risk\", \"seem\", \"great\", \"face\", \"today\", \"back\", \"enough\", \"turn\", \"table\", \"fall\", \"clear\", \"block\", \"live\", \"feel\", \"accept\", \"part\", \"idea\", \"ditch\", \"different\", \"fool\", \"truth\", \"big\", \"side\", \"explain\"], \"Freq\": [170.0, 648.0, 134.0, 593.0, 121.0, 117.0, 97.0, 381.0, 378.0, 68.0, 73.0, 72.0, 68.0, 60.0, 61.0, 57.0, 56.0, 261.0, 258.0, 46.0, 51.0, 47.0, 48.0, 45.0, 45.0, 40.0, 40.0, 43.0, 43.0, 43.0, 647.9785766601562, 593.25048828125, 380.8106384277344, 378.2111511230469, 260.6484375, 257.4450988769531, 203.0449981689453, 184.2433624267578, 181.61358642578125, 181.98695373535156, 163.65223693847656, 164.21499633789062, 156.7050323486328, 143.09085083007812, 128.7001953125, 125.87593078613281, 126.84429931640625, 120.55830383300781, 103.37751770019531, 89.30294036865234, 88.80744934082031, 84.86400604248047, 82.62627410888672, 79.740478515625, 77.31411743164062, 71.22705078125, 68.04794311523438, 67.32369232177734, 63.826412200927734, 62.97520446777344, 134.21737670898438, 72.59770965576172, 72.0104751586914, 61.16016387939453, 56.531009674072266, 55.31715774536133, 50.4652099609375, 47.49143981933594, 42.49409866333008, 42.35767364501953, 42.287513732910156, 41.373924255371094, 39.39666748046875, 39.875694274902344, 37.91004180908203, 35.33786392211914, 33.550350189208984, 29.131973266601562, 28.866687774658203, 28.960100173950195, 29.040630340576172, 24.72562599182129, 25.04193687438965, 25.181873321533203, 24.179800033569336, 22.68294334411621, 22.195064544677734, 21.128557205200195, 21.893129348754883, 20.554899215698242, 20.901628494262695, 169.61959838867188, 121.20397186279297, 116.57173919677734, 96.36277770996094, 67.3266830444336, 46.62613296508789, 45.115787506103516, 44.592132568359375, 42.56978225708008, 36.15559005737305, 33.88033676147461, 29.905399322509766, 28.46830177307129, 26.496904373168945, 26.602998733520508, 25.379392623901367, 26.221481323242188, 24.956850051879883, 24.295421600341797, 24.00897789001465, 21.92066764831543, 21.94795799255371, 21.02716636657715, 21.0787353515625, 20.55229377746582, 19.418291091918945, 19.78868293762207, 18.7359561920166, 20.188711166381836, 18.79861831665039, 19.4465274810791, 67.40328979492188, 59.8947868347168, 46.117645263671875, 39.902469635009766, 39.33698654174805, 37.31719970703125, 35.41672134399414, 34.88540267944336, 34.753055572509766, 34.61681365966797, 33.30149459838867, 33.606422424316406, 33.151527404785156, 28.88136863708496, 28.46394920349121, 26.564151763916016, 26.14876937866211, 25.497758865356445, 23.56454849243164, 23.180770874023438, 21.603208541870117, 21.222885131835938, 20.916088104248047, 20.69700813293457, 19.970762252807617, 20.048511505126953, 19.812711715698242, 19.391315460205078, 19.2165470123291, 18.682323455810547], \"Total\": [170.0, 648.0, 134.0, 593.0, 121.0, 117.0, 97.0, 381.0, 378.0, 68.0, 73.0, 72.0, 68.0, 60.0, 61.0, 57.0, 56.0, 261.0, 258.0, 46.0, 51.0, 47.0, 48.0, 45.0, 45.0, 40.0, 40.0, 43.0, 43.0, 43.0, 648.716796875, 593.9855346679688, 381.54888916015625, 378.9495849609375, 261.3867492675781, 258.18487548828125, 203.7790985107422, 184.97959899902344, 182.3455810546875, 182.72410583496094, 164.38304138183594, 164.95654296875, 157.43763732910156, 143.8226318359375, 129.43399047851562, 126.6129150390625, 127.59859466552734, 121.29850769042969, 104.1155776977539, 90.03327941894531, 89.5496826171875, 85.5990982055664, 83.35942840576172, 80.48090362548828, 78.04496002197266, 71.95348358154297, 68.77985382080078, 68.06520080566406, 64.56664276123047, 63.7183837890625, 134.96438598632812, 73.35482025146484, 72.79476165771484, 61.92532730102539, 57.28023910522461, 56.07187271118164, 51.218502044677734, 48.2418212890625, 43.25193786621094, 43.114601135253906, 43.044090270996094, 42.138675689697266, 40.151607513427734, 40.663509368896484, 38.67159652709961, 36.089271545410156, 34.316741943359375, 29.875619888305664, 29.612110137939453, 29.72027015686035, 29.810182571411133, 25.474159240722656, 25.803495407104492, 25.95052146911621, 24.930509567260742, 23.43103790283203, 22.949216842651367, 21.874547958374023, 22.668243408203125, 21.296995162963867, 21.681711196899414, 170.3691864013672, 121.94810485839844, 117.31977081298828, 97.11219024658203, 68.08211517333984, 47.371761322021484, 45.85579299926758, 45.345062255859375, 43.321998596191406, 36.89757537841797, 34.63583755493164, 30.672645568847656, 29.220544815063477, 27.2512149810791, 27.37024688720703, 26.13294792175293, 27.001934051513672, 25.70844841003418, 25.04070281982422, 24.76288604736328, 22.66228675842285, 22.710779190063477, 21.771753311157227, 21.827476501464844, 21.327651977539062, 20.16838836669922, 20.570087432861328, 19.478544235229492, 20.991464614868164, 19.546594619750977, 20.222490310668945, 68.15048217773438, 60.63676071166992, 46.85334396362305, 40.65632629394531, 40.087615966796875, 38.06998825073242, 36.156341552734375, 35.61844253540039, 35.487327575683594, 35.35427474975586, 34.033729553222656, 34.35004806518555, 33.898746490478516, 29.616661071777344, 29.212215423583984, 27.304719924926758, 26.885169982910156, 26.228864669799805, 24.307636260986328, 23.915061950683594, 22.349702835083008, 21.958148956298828, 21.650976181030273, 21.44757080078125, 20.711408615112305, 20.794565200805664, 20.56927490234375, 20.1323184967041, 19.962800979614258, 19.42699432373047], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.5992000102996826, -2.6875, -3.1308000087738037, -3.1375999450683594, -3.5099000930786133, -3.5223000049591064, -3.759700059890747, -3.856800079345703, -3.8712000846862793, -3.8691999912261963, -3.975399971008301, -3.97189998626709, -4.018700122833252, -4.109600067138672, -4.21560001373291, -4.237800121307373, -4.230100154876709, -4.281000137329102, -4.434700012207031, -4.581099987030029, -4.586599826812744, -4.631999969482422, -4.65880012512207, -4.694300174713135, -4.725200176239014, -4.807199954986572, -4.85290002822876, -4.86359977722168, -4.916900157928467, -4.9303998947143555, -2.6389000415802, -3.253499984741211, -3.2616000175476074, -3.4249000549316406, -3.5035998821258545, -3.5253000259399414, -3.6171000003814697, -3.677799940109253, -3.7890000343322754, -3.7922000885009766, -3.7939000129699707, -3.815700054168701, -3.8647000789642334, -3.85260009765625, -3.9031999111175537, -3.973400115966797, -4.025300025939941, -4.166600227355957, -4.1757001876831055, -4.172500133514404, -4.1697001457214355, -4.330599784851074, -4.317800045013428, -4.312300205230713, -4.35290002822876, -4.416800022125244, -4.438499927520752, -4.487800121307373, -4.452199935913086, -4.5152997970581055, -4.498600006103516, -2.3677000999450684, -2.7037999629974365, -2.742799997329712, -2.933199882507324, -3.2916998863220215, -3.65910005569458, -3.691999912261963, -3.703700065612793, -3.7500998973846436, -3.9133999347686768, -3.9783999919891357, -4.1031999588012695, -4.152500152587891, -4.2241997718811035, -4.220200061798096, -4.267300128936768, -4.2347002029418945, -4.28410005569458, -4.310999870300293, -4.322800159454346, -4.41379976272583, -4.412600040435791, -4.45550012588501, -4.453000068664551, -4.478300094604492, -4.535099983215332, -4.516200065612793, -4.570799827575684, -4.496200084686279, -4.567500114440918, -4.533599853515625, -3.0980000495910645, -3.216099977493286, -3.4774999618530273, -3.622299909591675, -3.6364998817443848, -3.689300060272217, -3.741499900817871, -3.7565999031066895, -3.7604000568389893, -3.764400005340576, -3.803100109100342, -3.7939999103546143, -3.8076000213623047, -3.945499897003174, -3.960099935531616, -4.029200077056885, -4.044899940490723, -4.070099830627441, -4.14900016784668, -4.16540002822876, -4.235899925231934, -4.253600120544434, -4.268199920654297, -4.27869987487793, -4.3144001960754395, -4.3105998039245605, -4.322400093078613, -4.343900203704834, -4.35290002822876, -4.381100177764893], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.46540001034736633, 0.4652999937534332, 0.46459999680519104, 0.46459999680519104, 0.46369999647140503, 0.46369999647140503, 0.4629000127315521, 0.4625999927520752, 0.4625000059604645, 0.4625000059604645, 0.46209999918937683, 0.4620000123977661, 0.461899995803833, 0.46140000224113464, 0.4609000086784363, 0.46070000529289246, 0.46059998869895935, 0.4603999853134155, 0.4593999981880188, 0.45840001106262207, 0.45820000767707825, 0.4578999876976013, 0.4577000141143799, 0.45730000734329224, 0.4571000039577484, 0.4564000070095062, 0.45579999685287476, 0.45559999346733093, 0.45500001311302185, 0.454800009727478, 1.9957000017166138, 1.9909000396728516, 1.990399956703186, 1.988800048828125, 1.9881000518798828, 1.9876999855041504, 1.9864000082015991, 1.9855999946594238, 1.9836000204086304, 1.9835000038146973, 1.9835000038146973, 1.9829000234603882, 1.982300043106079, 1.9816999435424805, 1.9814000129699707, 1.9802000522613525, 1.978700041770935, 1.9759999513626099, 1.9758000373840332, 1.9752999544143677, 1.975100040435791, 1.9714000225067139, 1.9713000059127808, 1.9711999893188477, 1.9707000255584717, 1.9687999486923218, 1.9678000211715698, 1.966599941253662, 1.9665000438690186, 1.9658000469207764, 1.9645999670028687, 2.0339999198913574, 2.0322000980377197, 2.0320000648498535, 2.030600070953369, 2.0271999835968018, 2.0225000381469727, 2.0220999717712402, 2.0216000080108643, 2.020900011062622, 2.0181000232696533, 2.0162999629974365, 2.013000011444092, 2.0123000144958496, 2.0102999210357666, 2.0099000930786133, 2.0090999603271484, 2.009000062942505, 2.008699893951416, 2.00819993019104, 2.007499933242798, 2.0051000118255615, 2.004199981689453, 2.0035998821258545, 2.003499984741211, 2.001300096511841, 2.000499963760376, 1.9996000528335571, 1.999500036239624, 1.999400019645691, 1.999400019645691, 1.9991999864578247, 2.219899892807007, 2.218600034713745, 2.215100049972534, 2.2121999263763428, 2.2119998931884766, 2.2109999656677246, 2.2102999687194824, 2.210099935531616, 2.2100000381469727, 2.2098000049591064, 2.209199905395508, 2.2090001106262207, 2.2086000442504883, 2.2058000564575195, 2.2049999237060547, 2.203399896621704, 2.203200101852417, 2.202699899673462, 2.199899911880493, 2.199700117111206, 2.197000026702881, 2.196899890899658, 2.1963999271392822, 2.1953001022338867, 2.194499969482422, 2.1944000720977783, 2.19350004196167, 2.1933999061584473, 2.1928000450134277, 2.191800117492676]}, \"token.table\": {\"Topic\": [4, 4, 4, 3, 4, 4, 3, 4, 2, 1, 3, 1, 2, 2, 1, 3, 2, 3, 3, 4, 1, 3, 1, 2, 1, 2, 1, 3, 1, 2, 4, 4, 1, 4, 4, 4, 2, 3, 4, 1, 4, 3, 4, 2, 2, 3, 2, 3, 1, 3, 4, 3, 1, 4, 2, 2, 2, 1, 1, 3, 3, 1, 4, 2, 3, 3, 1, 3, 2, 3, 2, 4, 3, 1, 4, 2, 1, 1, 3, 4, 2, 4, 2, 2, 3, 3, 3, 2, 2, 1, 3, 2, 2, 3, 2, 4, 2, 1, 3, 4, 3, 4, 4, 1, 1, 1, 2, 4, 2, 1, 1, 4, 1, 1, 4, 4, 1, 1, 2, 3, 3, 2], \"Freq\": [0.9843531250953674, 0.98311847448349, 0.9898093938827515, 0.9722856283187866, 0.9718942046165466, 0.9437561631202698, 0.968703031539917, 0.953148365020752, 0.963371753692627, 0.9956881999969482, 0.970775842666626, 0.9974941611289978, 0.9741479158401489, 0.981386661529541, 0.9938616752624512, 0.981337308883667, 0.9850573539733887, 0.9780701994895935, 0.9620901346206665, 0.9670758843421936, 0.9930011034011841, 0.9846372008323669, 0.9972202777862549, 0.9705207347869873, 0.9975390434265137, 0.9951075911521912, 0.9988950490951538, 0.9756738543510437, 0.9966470003128052, 0.9793290495872498, 0.9656513929367065, 0.9791318774223328, 0.9947043061256409, 0.9734873175621033, 0.9780205488204956, 0.9899792671203613, 0.9826333522796631, 0.9395479559898376, 0.9888400435447693, 0.9942012429237366, 0.9617369771003723, 0.956646740436554, 0.9617897868156433, 0.9836829304695129, 0.9890821576118469, 0.9724429845809937, 0.97105473279953, 0.9645525217056274, 0.9951591491699219, 0.9921522736549377, 0.9862675666809082, 0.942068338394165, 0.9953087568283081, 0.9699331521987915, 0.9860545992851257, 0.97576504945755, 0.9951629638671875, 0.9976698160171509, 0.9954107403755188, 0.969192385673523, 0.9864726662635803, 0.9892851710319519, 0.9873440265655518, 0.9762097001075745, 0.9885473847389221, 0.9972743391990662, 0.994280219078064, 0.9540858864784241, 0.9742584228515625, 0.9978330135345459, 0.9685582518577576, 0.983856737613678, 0.9816421866416931, 0.9843502640724182, 0.9563647508621216, 0.9728219509124756, 0.9985203742980957, 0.9961767792701721, 0.9754322171211243, 0.9894987940788269, 0.9600198268890381, 0.9817869067192078, 0.9626758694648743, 0.972977876663208, 0.9720363020896912, 0.9582298994064331, 0.9628939628601074, 0.9713185429573059, 0.9586383700370789, 0.9887256622314453, 0.9925673007965088, 0.9928545355796814, 0.9808839559555054, 0.9584395289421082, 0.9706911444664001, 0.9680182933807373, 0.9816039800643921, 0.9981048107147217, 0.9922253489494324, 0.9826369881629944, 0.9527682065963745, 0.9517702460289001, 0.9728690385818481, 0.9940246343612671, 0.9960371851921082, 0.9886615872383118, 0.9688609838485718, 0.9585031270980835, 0.9698172807693481, 0.991223931312561, 0.9885233640670776, 0.9696263074874878, 0.998561441898346, 0.9867486357688904, 0.972324013710022, 0.9791785478591919, 0.9983407855033875, 0.9866107702255249, 0.9757437109947205, 0.9923902750015259, 0.9841057062149048, 0.9907700419425964], \"Term\": [\"accept\", \"agree\", \"back\", \"beat\", \"become\", \"big\", \"blame\", \"block\", \"bori\", \"borisjohnson\", \"break\", \"brexit\", \"brexiteer\", \"bring\", \"campaign\", \"candidate\", \"chance\", \"choose\", \"clean\", \"clear\", \"come\", \"company\", \"conservative\", \"corrupt\", \"could\", \"country\", \"deal\", \"debate\", \"deliver\", \"dem\", \"different\", \"ditch\", \"election\", \"enough\", \"explain\", \"face\", \"fail\", \"fake\", \"fall\", \"farage\", \"feel\", \"fight\", \"fool\", \"force\", \"forget\", \"form\", \"full\", \"generalelection\", \"good\", \"government\", \"great\", \"hand\", \"happen\", \"idea\", \"important\", \"increase\", \"keep\", \"labour\", \"leave\", \"leaver\", \"let\", \"lie\", \"live\", \"look\", \"lose\", \"majority\", \"make\", \"move\", \"must\", \"need\", \"one\", \"option\", \"pact\", \"parliament\", \"part\", \"parti\", \"party\", \"people\", \"place\", \"plan\", \"police\", \"policy\", \"politic\", \"position\", \"possible\", \"power\", \"problem\", \"public\", \"put\", \"read\", \"real\", \"remain\", \"remainer\", \"remember\", \"report\", \"risk\", \"russian\", \"say\", \"seat\", \"seem\", \"serious\", \"side\", \"split\", \"stand\", \"stop\", \"support\", \"surrender\", \"table\", \"thank\", \"thing\", \"time\", \"today\", \"tory\", \"trade\", \"truth\", \"turn\", \"vote\", \"voter\", \"well\", \"win\", \"work\", \"wrong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el978817771997930887659777091\", ldavis_el978817771997930887659777091_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el978817771997930887659777091\", ldavis_el978817771997930887659777091_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el978817771997930887659777091\", ldavis_el978817771997930887659777091_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_data =  pyLDAvis.gensim.prepare(lda_model_n, corpus_n, id2word_n, mds='mmds')\n",
    "pyLDAvis.display(lda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with nlkt sentiment analysys\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_march['SENTIMENT_CP'] = cdf['TWEET_CLEANED'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df_march['SENTIMENT_NEUT'] = cdf['TWEET_CLEANED'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "cdf['SENTIMENT_NEG'] = cdf['TWEET_CLEANED'].apply(lambda x: sid.polarity_scores(x)['neg'])\n",
    "cdf['SENTIMENT_POS'] = cdf['TWEET_CLEANED'].apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "\n",
    "cdf.loc[cdf.SENTIMENT_CP > 0,'SENTIMENT'] = 'positive'\n",
    "cdf.loc[cdf.SENTIMENT_CP == 0,'SENTIMENT'] = 'neutral'\n",
    "cdf.loc[cdf.SENTIMENT_CP < 0,'SENTIMENT'] = 'negative'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
